%!TEX program = lualatex
\documentclass[aps,prl,twocolumn,groupedaddress,amsmath,amssymb]{revtex4-1}

\input{preamble}
\input{macros}

\begin{document}

    \title{Supplementing U-Nets with a GAN Loss for Cell Segmentation}
    \author{Erik Schultheis}
    \maketitle

	\section{Introduction}

    In this work a deep network called a U-Net \cite{ronneberger2015u} is used to generate
    segmentations from cell images. This means that given a black and white microscopy image of a
    slice of cells, we want the  network to detect the cell boundaries even if the quality of the
    original image is very low. To improve upon that architecture a regularizing loss inspired by
    generative adversarial  networks is added.

    The U-Net architecture was chosen due to its success for other bioimaging segmentation tasks. 
    It allows for precise localization of the segmentation classes and can be trained from few
    training examples. It is fully convolutional so it can be applied independent of the input
    image size. However, even the production of the low number (order of hundreds) of labelled
    training data can be expensive. 

    An alternative that does not need paired training data can be constructed with generative
    adversarial networks. In this case a \emph{Generator} network constructs fake data, and a
    \emph{Discriminator} network tries to distinguish it from real data. The competing  processes
    between generator and discriminator result in the generator producing more and more realistic
    data.

    By changing the generator to an image to image network, and introducing a cycle consistency
    loss, this architecture can be used for image-to-image transfers as done e.g. in
    \emph{CycleGANs} \cite{zhu2017unpaired} and \emph{DiscoGANs} \cite{kim2017learning}.  The
    training requires data from both domains, but does not need paired samples. In particular, one
    can use a data set that contains cell images for which no corresponding ground-truth
    segmentation is available.
    
    Unfortunatle, the training process of GAN networks is far more unstable and slow than that of 
    U-Nets. For this reason we try to combine these two processes by adding a GAN-like loss as an
    additional regularization to the U-Net. This should make the U-Net produce pictures that are 
    both close to the ground-truth data given a pixel-wise loss, and that are accepted as realistic
    by the discriminator.

    In the following we first build up the U-Net training process, taking a look at the effects
    of data augmentation, the choice of loss function, and the U-Net architecture. Then the GAN
    discriminator is introduced and calibrated so that it actually improves the training. This 
    turned out to require quite careful tuning of the discriminator parameters. We then look at the
    effects that the additional regularization has on the sample efficiency of the U-Net. Finally, 
    some shortcomings of the current approach are discussed and an outlook on possible future work 
    is given.

    \section{U-Net}

    In this section the basic U-Net model upon which the discriminator loss extension will be based,
    is developed. Several experiments are performed to find good settings for data augmentation, the
    loss function and the network architecture. Due to the high amount of hyperparameters, a
    comparison of all possible settings is not possible. Instead, one aspect (number of layers,
    loss function, data augmentation pipeline, \ldots) is varied and optimized while the others are
    kept fixed.

    \subsection{Architecture}
    The U-net architecture consists of resolution decreasing downward blocks, followed by resolution
    increasing upwards blocks. To improve the spatial precision, each upward block gets  as input
    both the result of the preceding block, and that of the corresponding downward block at the same
    resolution. As a downward block two convolutions with $3\times3$ receptive field are followed by
    a $2\times2$ max-pooling. With each deeper layer the  number of feature maps doubles. The
    convolutions are padded such that the resulting feature map has the same resolution as the
    original one.

    When increasing the spatial size of a feature layer by deconvolutions, the network is
    prone to generate so-called checkboard artifacts \cite{odena2016deconvolution}. While the 
    supervised nature of the U-Net training suppressed these problems, we still investigate whether
    performance can be improved by using a model inherently resistent to these effects. For that
    the deconvolutions are replaced by an upscaling, followed by a regular convolution. 

    The effects of different network depths (measured as the number of downward convolution blocks),
    and the choice of upscaling instead of deconvolution, are shown in figure~\ref{fig:arch}. As the
    results are only marginally different, the depth-4 upscaling network is chosen, as it may have
    additional, unused modelling capacity that could become useful when the additional loss is
    introduced below.

    \begin{figure}[tbp]
        \begin{subfigure}[c]{\linewidth}
            \includegraphics[width=\linewidth]{figures/arch.pdf}
            \caption{IoU}
        \end{subfigure}
        \begin{subfigure}[c]{\linewidth}
            \includegraphics[width=\linewidth]{figures/arch_xent.pdf}
            \caption{Cross-entropy}
        \end{subfigure}
        \caption{Comparison of different network architectures in terms of segmentation IoU and
        cross entropy. The performance of the different settings is comparable, except for the 
        4 layer deconvolution network. This may indicate an overfitting problem that can be
        prevented by either less parameters (depth 3) or by imposing additional structure on the 
        generation process (upscaling). The training loss is cross entropy and full data
        augmentation as described in the text has been used.}
        \label{fig:arch}
    \end{figure}

    \subsection{Data augmentation}
    Since there are only very few cell images and corresponding segmentations avaiable it is 
    necessary to make heavy use of data augmentation to facilitate training and generalization 
    of the network. The training data consists of 118 pairs of images, the evaluation data contains
    28 image pairs.

    The original cell images are given as 512 by 512 grayscale pixels. To reduce memory consumption
    these images are randomly cropped to $256\times256$. To increase the training set the images are
    randomly flipped (left-right and up-down) and rotated by multiples of 90 degrees, and the
    variability is further increased by random reductions of the image contrast by a factor in the
    range $[0.25, 1]$. These images are then downsampled to $128\times128$ to reduce the class
    imbalance. For cell images this corresponds to a $2\times2$ average pooling, whereas the binary
    segmentation images are resized with max pooling to preserve the binary structure.

    The positions of the cell boundaries may not be specified pixel-precise by the cell images. To 
    penalize shifts of the order of a few pixels not as much as missing boundary classification, one 
    can replace to hard, binary classification ground truth with a slightly blurred version (
    Gaussian filter with $\sigma=1$). To ensure that the central portion of the cell boundary still
    gets classified as $100\%$ cell boundary, the original ground truth is added to the blurred 
    version, with the result clipped at $1$.

    An even stronger increase in image variability can be archieved when the changes in contrast are
    not applied homogenously to the image (in which case they can mostly be reverted by an  image
    standartization), but with a locally varying strength. For this a 16 by 16 mask is created and
    upscaled to the full image size, blurred, and then used to interpolate between the image and a
    version whose contrast has been reduced to $10\%$ of the original value. The same approach can
    be used to apply local variations in image brightness. An example can be seen in
    figure~\ref{fig:augmentimage}.

    The effectiveness of this augmentation is demonstrated in figure~\ref{fig:augment}. Note that we
    cannot directly compare the effects on the evaluation loss, as the blurring of segmentations
    affects the loss calculation.

    \begin{figure}[tbp]
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{data/original.png}
            \caption{original}
        \end{subfigure}
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{data/augmented.png}
            \caption{preprocessed}
        \end{subfigure}
        \caption{Result of applying the full preprocessing procedure to a cell image. 
        Notice the brighter and darker patches caused by local contrast and brigthness changes.}
        \label{fig:augmentimage}
    \end{figure}

    \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/augment.pdf}
        \end{center}
        \caption{Effects of more extensive data preprocessing (additional blurring and local 
        contrast/brightness variations) vs only simple (crop, flip, rotate) augmentations on the 
        IoU of the generated segmentations over the learning process.}
        \label{fig:augment}
    \end{figure}

    \subsection{Loss function}
    A second design decision for the training process is the choice of the loss function. 
    Interpreting the segmentations as probability maps for a pixel being on a cell boundary frames
    this task as a pixel classification problem and leads to a pixelwise binary (sigmoid) cross 
    entropy loss. Thinking of the problem as an image-to-image transfer would suggest a pixelwise
    mean squared error as a simple loss function. 

    In both cases the strong class imbalance constitutes a problem: Most pixels in the target 
    segmentations are black, so a network that outputs all-black images would already get a high 
    accuracy. This can be countered by giving more weight to white pixels, which is automatically 
    done in the following way: The percentage of white $\bar{w}$ in the image is calculated 
    and the loss at a pixel of value $p$ is weighted as $w = p/\bar{w} + (1-p)/(1-\bar{w})$. If there
    are 9 times more black pixels than white, $\bar{w}=0.1$ and white pixels are weighted 9 times 
    stronger than black pixels.

    As demonstrated in figure~\ref{fig:mse} the cross-entropy and mse based training do perform
    about equally. Their difference is significantly smaller than the variability between runs. In
    the following we will use the cross-entropy loss, allowing for a probabilistic interpretation
    of the resulting segmentations.

    \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/mse.pdf}
        \end{center}
        \caption{Comparison of learning progress using the cross entropy and mean squared error 
        losses. White pixels (cell boundaries) are weighted more strongly than black pixels. 
        Within the given error bounds, both loss functions result in equivalent performance.}
        \label{fig:mse}
    \end{figure}

    \section{Regularisation by Discriminator}

    \subsection{Architecture}
    \begin{figure}[tbp]
        \begin{center}
        \input{figures/sketch}
        \end{center}
        \caption{Architecture of the U-Net. Bold arrows indicate weight updates by gradient descent. 
        The U-Net loss $L\ix{U}$ is based on a (pixel wise) comparison of $B$ and $B^{\prime}$. The
        discrimination losses are based on the discrimination results $d\ix{r} = D(B)$, 
        $d\ix{g}=D(B^{\prime})$. They are $L\ix{g, c} = \mathrm{xent}(d\ix{g}, 1)$, 
        $L\ix{d, f} = \mathrm{xent}(d\ix{g}, 0)$ and $L\ix{d, r} = \mathrm{xent}(d\ix{r}, 1)$.}
        \label{fig:unet_dics}
    \end{figure}
    
    The U-Net architcture above is now supplemented with a discriminator network to further refine
    the generated segmentations as shown in figure~\ref{fig:unet_dics}. The disciminator follows the
    architecture used in DCGANs \cite{dcgan}, i.e. it consists of blocks of a convolution followed
    by BatchNorm followed by a leaky ReLU. Since the U-Net is fully convolutional and thus can act
    on input images of arbitrary size, it made sense to also construct a fully convolutional
    discriminator, which is achieved by the last layer being a global average pooling into a single
    unit.

    As we want to train the discriminator on images that look somewhat realistic, 
    we only start training it after having trained the U-Net for 500 steps. The generator loss, in 
    turn, is only sensible for a sensible discriminator, so this additional loss is only enabled 
    after 750 steps. 

    The loss function for the generator consists of a cross entropy loss that makes the generator
    try to fool the discriminator, and a feature matching loss that tries to ensure that the
    features in the hidden layers of the discriminator follow the same statics (in this case mean),
    independently of whether the input comes from a real segmentation of from a generated one
    \cite{salimans2016improved}. The effect of the feature matching loss can be seen in 
    figure~\ref{fig:gan_fm}. 

     \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/gan_3_no_fm.pdf}
        \end{center}
        \caption{Comparison of learning progress with and without feature matching for the three
        layer UNet-GAN model. The differences between these two approaches are smaller than the 
        variability in different runs.}
        \label{fig:gan_fm}
    \end{figure}

    This loss is weighted with a factor $\lambda$ and then added to the pixel-wise U-Net loss for 
    training of the U-Net parameters.

    \subsection{Stabilizing the Training}

    Unfortunately the discriminator training can be very unstable. For a setting in which the GAN is
    supposed to only influence the training slightly $\lambda=0.1$, this results in a learning curve
    as depicted in figure~\ref{fig:gan_adam}. Though initially (at about $4000$ steps) better, the
    U-Net+GAN combination's performance soon starts to deteriorate strongly. This corresponds to a
    decrease in the discriminator loss and an \emph{increase} in the
    generator loss figure~\ref{fig:gdloss} -- it appears that the discriminator becomes too
    successfull. This  increases the generator loss, but the training seems to 1) no be able to
    reduce the generator  loss further and 2) not produce useful gradients from the discriminator.

    \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/disc_sgd_adam.pdf}
        \end{center}
        \caption{Adding a GAN loss makes the learning start out better, but leads to instability. 
        Using a much slower SGD discriminator prevents this from happening, but also no longer 
        improves performance.}
        \label{fig:gan_adam}
    \end{figure}

    A first approach to tackle that problem is by replacing the ADAM optimizer in the discriminator
    by SGD (with momentum $0.9$). In that case the discriminator almost does not improves at all,
    even if the learning rate is increased to $10^{-3}$ compared to $5\cdot10^{-5}$ in the ADAM
    case.  It improves stability (figure~\ref{fig:gan_adam}), but also removes the usable signal
    from the discriminator gradient: The generator loss remains very small (figure~\ref{fig:gdloss})
    and the IoU is slightly worse than without the GAN regularization.

    \begin{figure}[tbp]
        \begin{center}
        \begin{subfigure}[c]{\linewidth}
        \includegraphics[width=\linewidth]{figures/dloss_sgd_adam.pdf}
        \label{Discriminator loss}
        \end{subfigure}
        \begin{subfigure}[c]{\linewidth}
        \includegraphics[width=\linewidth]{figures/gloss_sgd_adam.pdf}
        \label{Generator loss (logarithmic scale)}
        \end{subfigure}
        \end{center}
        \caption{Comparison of generator and discriminator losses. SGD does not result in a significant
        reduction of discriminator loss, but the generator loss remains stable. This stabelizes the
        learning sbut also prevents the GAN from
        providing a helpful training signal.}
        \label{fig:gdloss}
    \end{figure}

    Therefore, other techniques are needed to ensure a good balance between generator and
    discriminator. One approach is to stop the discriminator training whenever the generator loss
    becomes to big, to give the generator a chance to catch up with the discriminator. Another  idea
    is to increase the relative weight $\lambda$ of the generator loss compared to the U-Net loss
    and limit the influence of this term by clipping to some upper bound.

    

    The results of these approaches can be seen in figure~\ref{fig:better_gan}, the behaviour of 
    generator and discriminator is depicted in figure~\ref{fig:better_gan_dgloss}. For 
    $\lambda=0.25$, $\mathrm{clipping}=0.25$ the GAN approach outperforms the pure U-Net by about 
    $1\%$ in IoU. 

    \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/disc_more.pdf}
        \end{center}
        \caption{Different approaches to balance GAN training. Either by changing the GAN loss 
        weight $\lambda$ and clipping, or by stopping the discriminator when the generator loss 
        becomes too high. A well calibrated GAN can improve the U-Net's performance.}
        \label{fig:better_gan}
    \end{figure}

    \begin{figure}[tbp]
        \begin{center}
        \begin{subfigure}[c]{\linewidth}
            \includegraphics[width=\linewidth]{figures/dloss_more.pdf}
            \caption{Discriminator loss}
        \end{subfigure}\\
        \begin{subfigure}[c]{\linewidth}
             \includegraphics[width=\linewidth]{figures/gloss_more.pdf}
            \caption{Generator loss}
        \end{subfigure}
        \end{center}
        \caption{Discriminator and genreator loss for the same data as in figure~\ref{fig:better_gan}.
        The generator and discriminator losses of the different methods are very similar, suggesting
        that the balance of generator and discriminator is only a necessary, but not sufficient
        condition for the GAN regularization to imporve performance.}
        \label{fig:better_gan_dgloss}
    \end{figure}


    Taking a look at the generated images fig.~\ref{fig:unet_vs_ganunet} reveals another problem: 
    The GAN loss seems to cause severe checkerboard artifacts in the generated image. Replacing the
    deconvolutions in the U-Net with upscaling layers fixes this problem. In this case, however, 
    using $\lambda=0.25$ makes the network too sensitive. At $\lambda=0.1$ is produces segmentations
    that have sharper lines than in the pure U-Net approach.

    \begin{figure}[tbp]
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{data/27_orig.png}
            \caption{Cell image}
        \end{subfigure} %
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{data/27_seg.png}
            \caption{Cell boundaries}
        \end{subfigure} \\
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{figures/27_upscaling.png}
            \caption{$\lambda=0$}
        \end{subfigure}%
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{figures/27_gan.png}
            \caption{$\lambda=0.25$}
        \end{subfigure}\\
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{figures/27_seg_upscaling.png}
            \caption{segmentation, \\ $\lambda=0.0$}
        \end{subfigure}%
        \begin{subfigure}[c]{0.45\linewidth}
            \includegraphics[width=\linewidth]{figures/27_seg_gan.png}
            \caption{segmentation, $\lambda=0.25$}
        \end{subfigure}
        \caption{Comparison of resulting image for the U-Net ($\lambda=0$) and U-Net with GAN loss.
         The additional discrimination loss causes the segmented cell boundaries
        to become more crisp, but also seems to add some artifacts.
        The last row shows the resulting segmentations (last row). These show that the improvement 
        in IoU is not due to a qualitatively better segmentation approach, but due to the better 
        matching of cell areas resulting from the sharper boundaries.}
        \label{fig:unet_vs_ganunet}
    \end{figure}

    Looking at the resulting learning curves figure~\ref{fig:gan_up} shows that the advantage 
    (in IoU) over pure U-Net is smaller than in the deconvolution approach. In particular the 
    $\lambda=0.25$ case reaches a point where there is a hugh performance drop (that might have been
    compensated again if the training had continued).

    \section{Sample Efficiency}

    A first experiment in sample efficiency can be performed by reducing the training set to only 20
    images. This drastic reduction to about one sixth of the original training data is handled
    rather well by the U-Net approach. While the IoU becomes significantly reduced, it still remains
    above $84\%$. The performance levels of after a few thousand steps, whereas for the full data 
    set 

    Adding the GAN loss improves the results slightly, see figure~\ref{fig:reduced_data}. Note that
    the same hyperparameters as for the full dataset were used; no fine tuning was performed. 

    This is in line with the previous results that the GAN training improves the stroke of the 
    cell boundaries, but does not help in detecting more cells.

    \begin{figure}[tbp]
        \begin{center}
        \includegraphics[width=\linewidth]{figures/less_data.pdf}
        \end{center}
        \caption{IoU when training only on 20 examples, with and without GAN loss. For reference
        also the behaviour on the full training set is shown. In this scenario the additional GAN
        loss provides some improvement.}
        \label{fig:reduced_data}
    \end{figure}


    \section{Discussion and Further Work}
    A significant problem in training and evaluation the segmentations is due to the quality of the
    ground truth data. As the network becomes good it starts to (correctly) mark more cell 
    boundaries at the outer fringes of the cell image. However, these lines are not part of the 
    ground-truth data (because the quality of the original image in these regions is really low), so
    the cross-entropy loss will train the network to not detect these boundaries. See for example 
    the left side of the images in figure~\ref{fig:unet_vs_ganunet}.

    This could be alleviated by providing an additional map that contains information about the 
    condidence of the ground truth data -- so that the cross entropy could be weighted less in low
    confidence regions.

    For the evaluation procedure this means that it is impossible for the loss to become smaller 
    (or the IoU larger) than some threshold that is determined by the data quality.

    Another possible area of improvement is the data augmentation pipeline. The current 
    augmentations have the advantage of transforming the segmentations pixel precise. If one lifts 
    this restriction, arbitrary rotations and shears, as well as slight scale variations, could be
    added as preprocessing modifications.

    As above demonstrated, the GAN loss can be used to improve the training when there are only few
    training examples. Going even further, one could modify the training procedure to only require 
    partially labeled data. For any data point for which no ground truth segmentation exists, the 
    U-Net should still generate a segmentation that the discriminator accepts as ``realistic''. 


    \section{Summary}

    A U-Net can be used to detect cell boundaries in microscopy images. By aggressive data
    augmentation the training can be further improved. Introducing an additional discriminator loss
    that judges produced segmentations for whether the seem ``realistic'' can further improve the 
    networks performance, but requires some fine-tuning and makes the training process more 
    unstable. In a setting where labelled data is extremely scarce, the regularisation provided
    by the discriminator becomes more useful.




    \bibliographystyle{plain}
    \bibliography{lit.bib} 
\end{document}

